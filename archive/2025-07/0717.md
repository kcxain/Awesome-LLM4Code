## Overview

1. GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities
    - 这篇论文介绍了一个名为GitChameleon的新基准测试，旨在评估人工智能代码生成系统在处理Python库版本不兼容性方面的能力。
2. Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization
    - 这篇研究论文介绍了一种名为Chain-of-Descriptions（CoDes）的新方法，旨在提高大型语言模型（LLMs）在VHDL代码生成和摘要任务中的性能。
3. MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks
    - 这篇论文介绍了MERA Code，一个专门针对俄罗斯语的最新代码生成大型语言模型（LLMs）的评估框架，旨在填补现有评估方法在代码质量和多语言支持方面的空白。
4. ExpliCIT-QA: Explainable Code-Based Image Table Question Answering
    - 这篇论文介绍了一个名为ExpliCIT-QA的系统，它能够处理复杂的表格图像，并提供可解释的答案，以提高视觉问答系统在表格数据上的透明度和可审计性。
5. MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization
    - 这篇论文介绍了一个名为MetaLint的新框架，它通过指令遵循和由易到难的泛化方法，提高了大型语言模型在代码质量分析方面的泛化能力和适应性。
6. The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs
    - 这篇论文揭示了基于扩散的大型语言模型（dLLMs）在面对上下文感知的、掩码输入的对抗性提示时存在的根本性安全问题，提出了一种新的攻击框架DIJA，展示了dLLMs在安全性方面的新漏洞。
7. Function-to-Style Guidance of LLMs for Code Translation
    - 这篇论文讲述了如何通过功能到风格的引导范式（F2STRANS），提升大型语言模型（LLMs）在代码翻译任务中的性能，包括代码的正确性和可读性。
8. CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks
    - 这篇论文介绍了一个名为CodeJudgeBench的基准测试，旨在评估大型语言模型（LLM）在编程任务中作为裁判（Judge）的性能。
9. CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance
    - 这篇论文介绍了CodeAssistBench（CAB），一个用于评估多轮对话式编程辅助的大型语言模型（LLMs）的基准框架，它通过自动化数据生成和模拟真实用户交互来解决现有基准的局限性。
10. A Code Comprehension Benchmark for Large Language Models for Code
    - 这篇论文探讨了大型语言模型在代码理解任务中的表现，并提出了通过在大规模数据集上微调这些模型来提高代码理解能力的方法。
11. Turning the Tide: Repository-based Code Reflection
    - 这篇论文介绍了一个名为LiveRepoReflection的挑战性基准测试，旨在评估代码大型语言模型（LLMs）在多文件仓库环境中理解和生成代码的能力，并提出了一个基于对话的训练数据集RepoReflection-Instruct，用于提升模型在实际编程任务中的表现。
12. A Mixture of Linear Corrections Generates Secure Code
    - 这篇论文讲述了如何通过一种混合线性校正（MoC）的方法，引导大型语言模型（LLMs）生成更安全的代码，同时不牺牲代码的功能。
13. OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique
    - 这篇论文介绍了一个名为OpenCodeReasoning-II的新数据集，以及通过自批评测试时扩展方法来提升代码生成和批判性能的研究。
14. On Evaluating Performance of LLM Inference Serving Systems
    - 这篇论文讲述了如何系统地评估大型语言模型（LLM）推理系统的性能，并提出了一套新的评估方法和框架，以解决现有评估方法中存在的问题。
15. BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity
    - 这篇论文介绍了一种新型的Mixture-of-Experts（MoE）架构，名为BlockFFN，旨在提高大型语言模型（LLMs）在资源受限的终端设备上的加速性能，通过实现块级别的激活稀疏性。
16. Multilingual Multimodal Software Developer for Code Generation
    - 这篇论文介绍了一个名为M2-CODER的多语言多模态软件开发者，它能够结合文本指令和视觉设计输入（如UML图和流程图）来提高代码生成的准确性和与架构对齐。
17. Agentic Large Language Models for Conceptual Systems Engineering and Design
    - 这篇论文探讨了如何利用代理型大型语言模型（LLM）在概念性系统工程和设计中进行需求提取、功能分解和模拟器代码生成。
18. Automating MD simulations for Proteins using Large language Models: NAMD-Agent
    - 这篇论文介绍了一个利用大型语言模型（LLMs）自动化分子动力学（MD）模拟输入文件生成的管道，特别是针对蛋白质模拟，通过结合Python脚本和基于Selenium的网页自动化来简化NAMD的输入文件准备过程。




### GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities

**作者**: Diganta Misra, Nizar Islah, Victor May, Brice Rauby, Zihan Wang, Justine Gehring, Antonio Orvieto, Muawiz Chaudhary, Eilif B. Muller, Irina Rish, Samira Ebrahimi Kahou, Massimo Caccia
**日期**: 2025-07-16
**链接**: http://arxiv.org/abs/2507.12367v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个名为GitChameleon的新基准测试，旨在评估人工智能代码生成系统在处理Python库版本不兼容性方面的能力。

2. 摘要翻译：
软件库的快速发展为代码生成带来了巨大挑战，需要不断适应频繁的版本更新，同时保持向后兼容性。尽管现有的代码演化基准测试提供了宝贵的见解，但它们通常缺乏基于执行的评估，以生成符合特定库版本的代码。为了解决这个问题，我们引入了GitChameleon，这是一个新颖的、精心策划的数据集，包含328个基于特定库版本的Python代码补全问题，并附带可执行的单元测试。GitChameleon严格评估了当代大型语言模型（LLMs）、LLM驱动的代理、代码助手和RAG系统在执行功能准确性方面进行版本条件代码生成的能力。我们广泛的评估表明，最先进的系统在这项任务上遇到了重大挑战；企业模型在基线成功率上达到了48-51%的范围，突显了问题的复杂性。通过提供一个强调代码库动态特性的基于执行的基准测试，GitChameleon有助于更清晰地理解这一挑战，并指导开发更具适应性和可靠性的AI代码生成方法。我们公开提供数据集和评估代码。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提出了GitChameleon，一个包含328个基于Python的版本条件问题的新型代码补全基准测试，包括用于自我调试的可见测试和文档引用，用于检索增强生成（RAG）。
- 对GitChameleon进行了全面的实证研究，评估了多种当代AI代码生成系统的能力，包括AI代理、集成开发环境（IDE）和命令行界面（CLI）基础的编码助手，以及基于RAG的LLM管道。
- 揭示了当前AI系统在遵守特定版本约束方面的关键局限性，并强调了影响其性能的因素，为开发更具适应性和可靠性的AI代码生成方法提供了见解。
动机和解决的问题是：现有的代码演化基准测试通常关注于评估LLMs的代码演化或迁移能力，这些能力通常只向前发展，并且涉及在训练期间未遇到的库版本或全新的库。这种框架本质上使任务成为分布外（OOD）的任务。相比之下，版本条件生成（VCG）——LLMs生成与特定先前见过的库版本一致的代码的能力——对于实际部署至关重要。它使模型能够在实际生产环境或受限设置中可靠地运行，这些环境中使用的库可能不是最新的稳定版本。

4. 方法，具体流程：
GitChameleon的方法包括：
- 手动编写包含328个基于Python的版本条件问题，重点关注流行的代码库。
- 每个问题都附带一系列断言基础的单元测试，允许对潜在解决方案进行全面的基于执行的评估。
- 数据集样本包括与Python库中的重大变更相关的问题，并提供一套测试，包括用于模型性能评估和排名的全面隐藏测试，以及提供执行反馈的简洁可见测试。
- 评估流程包括问题陈述、起始代码、依赖信息、候选解决方案、隐藏测试、可见测试和验证输入。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：GitChameleon包含328个基于Python的版本条件问题，涵盖26个库，包括科学计算、数据科学和Web开发。样本收集自2014年至2023年的版本发布，不包括遗留和撤回的版本发布。
实验设置：评估了多种设置，包括贪婪解码、束搜索和自调试。
实验结果：企业模型在基线成功率上达到了48-51%的范围，显示了现有系统在处理库版本兼容性方面的挑战。
实验结论：GitChameleon揭示了当前AI系统在遵守特定版本约束方面的关键局限性，并强调了影响其性能的因素。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
GitChameleon的方法可以应用于其他领域，如：
- 代码生成：可以用于生成特定于库版本的代码，例如Verilog代码生成，确保生成的代码与特定版本的硬件描述语言库兼容。
- 代码修复：可以帮助识别和修复因库版本不兼容而导致的错误，提高代码的健壮性和兼容性。
- CoT（Chain of Thought）：可以用于开发和评估基于CoT的AI系统，这些系统需要理解和处理不同库版本之间的差异，以生成正确的代码解决方案。

---

### Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization

**作者**: Prashanth Vijayaraghavan, Apoorva Nitsure, Charles Mackin, Luyao Shi, Stefano Ambrogio, Arvind Haran, Viresh Paruthi, Ali Elzein, Dan Coops, David Beymer, Tyler Baldwin, Ehsan Degan
**日期**: 2025-07-16
**链接**: http://arxiv.org/abs/2507.12308v1

1. 一句话介绍论文讲的故事：
这篇研究论文介绍了一种名为Chain-of-Descriptions（CoDes）的新方法，旨在提高大型语言模型（LLMs）在VHDL代码生成和摘要任务中的性能。

2. 摘要翻译：
大型语言模型（LLMs）在多种自然语言处理任务和领域中得到了广泛应用，显示出它们的适应性和有效性。在电子设计自动化（EDA）领域，LLMs在寄存器传输级（RTL）代码生成和摘要等任务上显示出潜力。然而，尽管LLMs在通用代码相关任务中得到了广泛应用，但针对硬件描述语言（HDLs），尤其是VHDL的研究却相对缺乏。在本研究中，我们使用各种指标和两个数据集——VHDL-Eval和VHDL-Xform——评估现有代码LLMs在VHDL代码生成和摘要任务中的表现。VHDL-Xform是一个内部数据集，旨在衡量LLMs对功能等价代码的理解。我们的发现揭示了这些模型在不同指标上的一致性不佳，强调了它们在这一领域的适用性存在显著差距。为了解决这一挑战，我们提出了Chain-of-Descriptions（CoDes），这是一种新的方法，用于LL提高Ms在VHDL代码生成和摘要任务中的性能。CoDes涉及基于问题陈述生成一系列中间描述步骤（对于代码生成）和基于VHDL代码生成中间描述步骤（对于摘要）。然后，这些步骤与原始输入提示（问题陈述或代码）集成，并作为输入提供给LLMs以生成最终输出。我们的实验表明，CoDes方法在两个数据集的各种指标上显著优于标准提示策略。这种方法不仅提高了VHDL代码生成和摘要的质量，而且为未来旨在增强VHDL代码LLMs的研究提供了一个框架。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提出了Chain-of-Descriptions（CoDes）方法，通过生成基于问题陈述或VHDL代码的中间描述步骤，以提高LLMs在VHDL代码生成和摘要任务中的性能。
- 引入了VHDL-Xform数据集，包含多种代码克隆，旨在评估LLMs对功能等价代码的理解。
- 对多种LLMs进行了零样本评估，使用VHDL-Eval和VHDL-Xform数据集进行VHDL代码生成和摘要。
- 通过CoDes策略增强LLMs在VHDL代码生成和摘要中的性能，为未来LLMs在VHDL领域的研究建立了基准。

动机和解决的问题：
- 现有的LLMs在VHDL代码生成和摘要任务中表现不佳，存在显著的性能差距。
- LLMs缺乏针对VHDL的具体调整，因为它们对VHDL数据的暴露有限，且缺乏对这种语言的健壮评估方法。
- LLMs在理解功能等价性方面存在困难，导致在代码摘要中出现差异，它们经常优先考虑语法相似性而非功能等价性。

4. 方法，具体流程：
CoDes方法的具体流程如下：
- 根据任务类型（代码生成或摘要），生成一系列中间描述步骤。
- 对于代码生成，基于问题陈述生成中间步骤；对于摘要，基于VHDL代码生成中间步骤。
- 将这些中间步骤与原始输入提示（问题陈述或代码）集成。
- 将集成后的输入提供给LLMs，以生成最终的代码或摘要输出。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：
- VHDL-Eval：由202个问题组成，是通过将Verilog评估问题转换为VHDL并编译公开可用的VHDL问题而创建的。
- VHDL-Xform：利用公开的Github VHDL代码库，基于代码克隆的概念（代码块与另一代码块相同或非常相似）应用不同的代码转换策略。

实验设置：
- 使用VHDL-Eval和VHDL-Xform数据集对现有的代码LLMs进行评估。
- 评估指标包括pass@k指标、自一致性得分、ROUGE-L和LLM偏好率（PR）。

实验结果：
- CoDes方法在两个数据集的各种指标上显著优于标准提示策略。

实验结论：
- CoDes方法提高了VHDL代码生成和摘要的质量，并为未来增强VHDL代码LLMs的研究提供了一个框架。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
CoDes方法可以应用于其他领域，包括：
- 代码生成：尤其是Verilog代码生成，因为

---

### MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks

**作者**: Artem Chervyakov, Alexander Kharitonov, Pavel Zadorozhny, Adamenko Pavel, Rodion Levichev, Dmitrii Vorobev, Dmitrii Salikhov, Aidar Valeev, Alena Pestova, Maria Dziuba, Ilseyar Alimova, Artem Zavgorodnev, Aleksandr Medvedev, Stanislav Moiseev, Elena Bruches, Daniil Grebenkin, Roman Derunets, Vikulov Vladimir, Anton Emelyanov, Dmitrii Babaev, Vladimir V. Ivanov, Valentin Malykh, Alena Fenogenova
**日期**: 2025-07-16
**链接**: http://arxiv.org/abs/2507.12284v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了MERA Code，一个专门针对俄罗斯语的最新代码生成大型语言模型（LLMs）的评估框架，旨在填补现有评估方法在代码质量和多语言支持方面的空白。

2. 摘要翻译：
在大型语言模型（LLMs）的进步推动下，软件工程任务自动化得到了显著提升；然而，当前的评估方法主要关注自然语言任务，忽视了代码质量。大多数基准测试优先考虑高层次推理而非可执行代码和现实世界性能，这在理解这些模型的真实能力和生产环境中与之相关的风险方面留下了空白。为了解决这个问题，我们提出了MERA Code，它是MERA基准测试家族的新成员，专门针对评估俄罗斯语的最新代码生成LLMs。这个基准测试包括11个评估任务，涵盖8种编程语言。我们提出的评估方法采用了一个分类体系，概述了模型完成这些任务所需的实用编程技能。基准测试包括一个开源代码库，供用户进行MERA评估，一个与各种编程环境兼容的评分系统，以及一个具有排行榜和提交系统的平台。我们评估了开放的LLMs和前沿API模型，分析了它们在非英语语言的实际编程任务中的局限性。我们公开发布MERA，以指导未来的研究，预测模型开发中的突破性功能，并标准化评估程序。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提出了一个可复现的俄罗斯语LLMs评估方法；
- 设计了11个指令格式化的任务（code2text、text2code、code2code），覆盖8种编程语言（Python、Java、C#、JavaScript、Go、C、C++和Scala）；
- 提供了一个开放的评估平台，包含评分系统、框架和公共排行榜；
- 对开源通用模型到专有编码API的性能进行了分析。
动机是填补现有评估方法在代码质量和多语言支持方面的空白，解决的问题是现有基准测试未能全面捕捉自然语言与代码之间的交互，特别是在非英语环境中。

4. 方法，具体流程：
MERA Code的方法包括以下几个步骤：
- 定义了一个基于软件工程领域的分类体系，将任务分解为有限数量的技能；
- 为每个任务设计了一组不同的提示，以确保评估的无偏性和鲁棒性；
- 实施了一个评估程序，模型持续生成令牌直到满足预定义的停止条件；
- 对所有任务采用纯生成性和指导性评估，模型输出经过特定任务的后处理以符合度量要求；
- 使用多种度量标准，如Pass@k、Compile@k、chrF、BLEU、CodeBLEU、Exact Match和Judge@k，来评估生成代码的功能正确性、编译正确性、文本相似度等。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
实验结果部分，论文提到了MERA Code包含11个代码任务，并提供了任务的统计数据。例如，CodeLinterEval任务评估Python代码生成和修正能力，基于linter错误，包含110个样本；CodeCorrectness任务评估LLMs预测单元测试是否编译和执行成功或失败的能力，包含1361个样本。实验设置包括了对开源通用模型到专有编码API的性能分析。具体的实验结果和结论没有在摘要中提供，需要查看论文的实验部分以获取详细信息。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
MERA Code的方法可以应用于其他领域，包括但不限于：
- 代码生成：可以用于评估生成特定领域代码（如Verilog）的能力；
- 代码修复：评估模型修复代码中错误和缺陷的能力；
- CoT（Chain of Thought）：评估模型在解决复杂问题时的逻辑推理和步骤分解能力。由于MERA Code涵盖了多种编程语言和任务类型，其评估框架和方法论可以为这些领域提供标准化的评估工具和基准。

---

### ExpliCIT-QA: Explainable Code-Based Image Table Question Answering

**作者**: Maximiliano Hormazábal Lagos, Álvaro Bueno Sáez, Pedro Alonso Doval, Jorge Alcalde Vesteiro, Héctor Cerezo-Costas
**日期**: 2025-07-15
**链接**: http://arxiv.org/abs/2507.11694v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个名为ExpliCIT-QA的系统，它能够处理复杂的表格图像，并提供可解释的答案，以提高视觉问答系统在表格数据上的透明度和可审计性。

2. 摘要翻译：
我们提出了ExpliCIT-QA，这是一个扩展了我们之前MRT方法的系统，用于处理复杂的表格图像并提供可解释的答案。ExpliCIT-QA遵循模块化设计，包括：(1)多模态表格理解，使用链式思考方法从表格图像中提取和转换内容；(2)基于语言的推理，生成逐步的自然语言解释来解决问题；(3)自动代码生成，基于推理步骤创建Python/Pandas脚本，并提供错误处理反馈；(4)代码执行，计算最终答案；以及(5)自然语言解释，描述如何计算答案。系统构建为透明和可审计：所有中间输出、解析表格、推理步骤、生成代码和最终答案都可供检查。这种策略旨在缩小端到端TableVQA系统中的可解释性差距。我们在TableVQA-Bench基准上评估了ExpliCIT-QA，并与现有基线进行了比较。我们展示了在可解释性和透明度方面的改进，为金融和医疗等敏感领域的应用打开了大门，这些领域中审计结果至关重要。代码可在https://github.com/maxhormazabal/ExpliCIT上找到。

3. 主要贡献和创新点，动机和解决的问题：
ExpliCIT-QA的主要贡献和创新点在于：
- 提出了一个新颖的多步骤VLLM和LLM推理集成的管道，用于视觉问答中的表格图像。
- 强调了在TableVQA中透明度的重要性，系统为每个答案提供了人类可读的链式思考解释和可运行的代码，解决了端到端模型解释的模糊性问题。
- 在TableVQA-Bench数据集上进行了实验，与基线模型进行了比较，展示了ExpliCIT-QA在可解释性和透明度方面的优势。

动机是解决现有视觉问答系统中缺乏透明度和可解释性的问题，特别是在金融和医疗等高风险领域，用户需要信任和验证答案的推导过程。

4. 方法，具体流程：
ExpliCIT-QA的方法包括五个主要模块，顺序操作：
(1) 多模态表格理解：使用Qwen-2.5-VL处理表格图像，提取内容和结构。
(2) 基于语言的推理：使用链式思考方法生成逐步解释，解决问答问题。
(3) 自动代码生成：基于推理步骤创建Python/Pandas脚本，并提供错误处理反馈。
(4) 代码执行：运行代码计算最终答案。
(5) 自然语言解释：描述如何计算答案，提供可审核的输出。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：TableVQA-Bench
实验设置：将ExpliCIT-QA与现有基线模型进行比较。
实验结果：ExpliCIT-QA在可解释性和透明度方面取得了改进。
实验结论：ExpliCIT-QA通过提供人类可读的解释和可运行的代码，提高了视觉问答系统在表格数据上的透明度和可审计性，为金融和医疗等敏感领域的应用提供了可能。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
ExpliCIT-QA的方法可以应用于其他领域，例如：
- 代码生成：通过自动代码生成模块，可以将类似的推理过程应用于生成其他类型的代码，如Verilog代码。
- 代码修复：系统的错误处理反馈机制可以用于代码修复，自动识别和修复代码中的错误。
- CoT（链式思考）：ExpliCIT-QA中的链式思考方法可以应用于其他需要逐步推理的任务，如逻辑推理、数学问题解决等，提高模型的可解释性和透明度。

---

### MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization

**作者**: Atharva Naik, Lawanya Baghel, Dhakshin Govindarajan, Darsh Agrawal, Daniel Fried, Carolyn Rose
**日期**: 2025-07-15
**链接**: http://arxiv.org/abs/2507.11687v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个名为MetaLint的新框架，它通过指令遵循和由易到难的泛化方法，提高了大型语言模型在代码质量分析方面的泛化能力和适应性。

2. 摘要翻译：
大型语言模型（LLM）虽然在代码生成方面取得了成功，但在代码质量分析方面却面临挑战，因为它们受限于静态训练数据，且难以适应不断演变的最佳实践。我们提出了MetaLint，这是一个新的指令遵循框架，它将代码质量分析任务定义为基于高级规范检测和修复有问题的语义代码片段或代码习语。与传统的训练模型不同，MetaLint通过在合成的linter生成数据上进行指令调整，支持由易到难的泛化，使模型能够适应新的或复杂的代码模式而无需重新训练。为了评估这一点，我们构建了一个基于真实世界编码标准（如Python增强提案PEPs）的挑战性习语基准，并评估MetaLint训练的模型是否能够适应性地推理或仅仅是记忆。我们的结果显示，MetaLint在未见过的PEP习语泛化方面有所改进，在习语检测上达到了70.37%的F分数，并且在所有评估的模型中具有最高的召回率（70.43%）。它还在定位上达到了26.73%，对于其4B参数大小而言是具有竞争力的，并且与更大的最新模型如o3-mini相当，突出了其在代码质量分析方面的潜力。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献包括：(1) 提出了MetaLint框架，它利用合成数据进行由易到难的泛化，解决了以往基于LLM的代码质量方法中静态训练的限制；(2) 构建了一个基于各种PEPs的挑战性习语基准，并评估了最新的代码和推理模型；(3) 分析了MetaLint是否促进了适应性推理而不是记忆，并实现了从易到难习语的泛化。动机是开发能够识别代码质量问题的LLM系统，而无需对目标习语进行显式监督，特别是对于难以处理或罕见的模式，并且能够随着最佳实践的演变而适应。

4. 方法，具体流程：
MetaLint的方法是将代码质量分析任务定义为元linting，即给定一个最佳实践代码习语I的规范，模型学习识别和定位违反习语的代码片段，并可以重写以遵守或融入习语I。核心思想是教会LLM基于高级规范识别和定位习语的结构，而不是训练它记忆或学习一组静态的最佳实践。MetaLint通过指令调整在合成的linter生成数据上进行训练，以支持由易到难的泛化。具体流程包括：(1) 使用现有linter生成大规模合成数据进行指令微调；(2) 在这些数据上训练模型，以处理更抽象、更复杂的习语；(3) 在基于PEPs构建的挑战性习语基准上评估模型。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：基于流行的PEPs构建了一个挑战性习语基准。
实验设置：评估了最新的代码和推理模型，并与MetaLint训练的模型进行了比较，重点关注LLM是否能够超越记忆，从易到难习语泛化，并适应新的模式。
实验结果：MetaLint在未见过的PEP习语泛化方面有所改进，在习语检测上达到了70.37%的F分数，并且在所有评估的模型中具有最高的召回率（70.43%）。它还在定位上达到了26.73%，对于其4B参数大小而言是具有竞争力的，并且与更大的最新模型如o3-mini相当。
实验结论：MetaLint通过指令遵循和由易到难的泛化，提高了模型在代码质量分析方面的泛化能力和适应性。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
MetaLint的方法可以应用于其他领域，如代码生成（尤其是Verilog代码生成），代码修复和上下文感知的代码生成（CoT）。由于MetaLint侧重于提高模型的指令遵循能力，而不是针对特定数据分布的最佳实践进行定制，因此这种方法可以适应新的习语和不断演变的最佳实践。在代码生成领域，MetaLint可以帮助模型理解和遵循高级代码规范，生成更高质量的代码。在代码修复领域，MetaLint可以识别和修复违反最佳实践的代码片段。在CoT领域，MetaLint可以帮助模型理解和遵循上下文相关的代码规范，生成更符合上下文要求的代码。

---

### The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs

**作者**: Zichen Wen, Jiashu Qu, Dongrui Liu, Zhiyuan Liu, Ruixi Wu, Yicun Yang, Xiangqi Jin, Haoyun Xu, Xuyang Liu, Weijia Li, Chaochao Lu, Jing Shao, Conghui He, Linfeng Zhang
**日期**: 2025-07-15
**链接**: http://arxiv.org/abs/2507.11097v1

1. 一句话介绍论文讲的故事：
这篇论文揭示了基于扩散的大型语言模型（dLLMs）在面对上下文感知的、掩码输入的对抗性提示时存在的根本性安全问题，提出了一种新的攻击框架DIJA，展示了dLLMs在安全性方面的新漏洞。

2. 摘要翻译：
基于扩散的大型语言模型（dLLMs）最近作为一种强大的替代方案出现，与传统的自回归大型语言模型（A-LLMs）相比，它们通过并行解码和双向建模提供了更快的推理和更大的交互性。然而，尽管dLLMs在代码生成和文本填充方面表现出色，我们发现了一个基本的安全问题：现有的对齐机制未能保护dLLMs免受上下文感知的、掩码输入的对抗性提示的影响，暴露出新的漏洞。为此，我们提出了DIJA，这是第一个系统性研究和越狱攻击框架，利用dLLMs的独特安全弱点。具体来说，我们提出的DIJA构建了对抗性的交错掩码-文本提示，利用dLLMs的文本生成机制，即双向建模和并行解码。双向建模驱使模型为掩码跨度产生上下文一致的输出，即使这些输出是有害的，而并行解码限制了模型动态过滤和拒绝采样不安全内容的能力。这导致标准对齐机制失败，使得即使在提示中直接暴露有害行为或不安全指令，也能在对齐调整的dLLMs中启用有害完成。通过全面的实验，我们展示了DIJA显著优于现有的越狱方法，暴露了dLLM架构中以前被忽视的威胁面。值得注意的是，我们的方法在Dream-Instruct上实现了高达100%的基于关键词的ASR，超过了最强的先前基线ReNeLLM，在JailbreakBench上的基于评估器的ASR上提高了78.5%，在StrongREJECT得分上提高了37.7分，同时不需要在越狱提示中重写或隐藏有害内容。我们的发现强调了重新思考这一新兴类别语言模型的安全对齐的迫切需要。代码可在https://github.com/ZichenWen1/DIJA找到。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：首次对dLLMs的安全问题进行调查，识别并描述了针对dLLMs的新攻击路径，这些攻击路径根植于它们的双向和并行解码机制；提出了DIJA，一个自动化的越狱攻击流程，能够将普通的越狱提示转换为交错的文本-掩码越狱提示，从而在dLLMs上引发有害的完成；通过在多个dLLMs上的广泛评估，展示了DIJA在绕过对齐安全措施方面的有效性，揭示了非自回归架构独有的以前被忽视的漏洞类别。动机是解决dLLMs在安全性方面的新挑战，特别是在面对上下文感知的、掩码输入的对抗性提示时的脆弱性。

4. 方法，具体流程：
DIJA的方法利用dLLMs的两个核心属性：双向上下文建模和并行解码，构建包含恶意意图的对抗性提示，迫使模型以上下文一致（可能有害）的方式完成掩码跨度。具体流程包括：设计一个自动化流程，将现有的有害提示转换为交错的文本-掩码变体，使用语言模型通过上下文学习指导提示的细化；利用dLLMs在推理期间无法动态过滤不安全生成的弱点，实现高攻击成功率，即使在对齐调整的dLLMs上也是如此。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
实验使用了多个公开的dLLMs，并在多个越狱基准上进行了广泛的评估。实验结果显示DIJA能够一致地绕过对齐安全措施，揭示了非自回归架构独有的以前被忽视的漏洞类别。具体来说，DIJA在Dream-Instruct上实现了高达100%的基于关键词的ASR，超过了最强的先前基线ReNeLLM，在JailbreakBench上的基于评估器的ASR上提高了78.5%，在StrongREJECT得分上提高了37.7分。实验结论是DIJA有效地展示了dLLMs在安全性方面的新漏洞，并强调了重新思考这一新兴类别语言模型的安全对齐的迫切需要。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
DIJA方法可以应用于需要安全性考量的任何领域，尤其是在需要防范对抗性攻击和确保输出内容安全的场景中。例如，在代码生成领域，尤其是Verilog代码生成中，DIJA可以帮助识别和防范可能生成有害代码的攻击；在代码修复领域，DIJA可以用于

---

### Function-to-Style Guidance of LLMs for Code Translation

**作者**: Longhui Zhang, Bin Wang, Jiahao Wang, Xiaofeng Zhao, Min Zhang, Hao Yang, Meishan Zhang, Yu Li, Jing Li, Jun Yu, Min Zhang
**日期**: 2025-07-15
**链接**: http://arxiv.org/abs/2507.11083v1

1. 一句话介绍论文讲的故事：
这篇论文讲述了如何通过功能到风格的引导范式（F2STRANS），提升大型语言模型（LLMs）在代码翻译任务中的性能，包括代码的正确性和可读性。

2. 摘要翻译：
大型语言模型（LLMs）在代码翻译任务中取得了显著进展。然而，确保翻译代码的正确性和可读性仍然是一个挑战，限制了它们在实际软件开发中的有效应用。在这项工作中，我们提出了F2STRANS，一个功能到风格的引导范式，旨在逐步提高LLMs在代码翻译中的性能。我们的方法包括两个关键阶段：（1）功能学习，使用从在线编程平台挖掘的高质量源代码-目标代码对来优化翻译的正确性；（2）风格学习，通过结合正面和负面的风格示例来提高翻译的可读性。此外，我们引入了一个新颖的代码翻译基准，包括最新的源代码、广泛的测试用例和手动标注的真实翻译，使得可以进行全面的功能和风格评估。实验表明，我们的方法显著提高了代码翻译性能。特别是，我们的方法使Qwen1.5B在20种不同的代码翻译场景中平均性能超过了提示增强的Qwen32B和GPT-4。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：提出了F2STRANS框架，一个功能到风格的引导框架，旨在提高LLMs生成的代码翻译的正确性和可读性；创建了一个全面的基准，用于严格评估代码翻译的功能准确性和风格一致性；在20种翻译场景中显著提高了各种类型和规模的LLMs翻译代码的质量。动机和解决的问题是：现有的LLMs在代码翻译中面临正确性和可读性的关键限制，这限制了它们在实际软件开发中的应用。F2STRANS通过功能学习和风格学习两个阶段来解决这些问题。

4. 方法，具体流程：
F2STRANS采用两阶段渐进学习范式。首先，通过功能一致的代码对进行功能学习，确保功能一致的代码翻译。其次，为了提高源代码和目标代码之间的风格对齐，提出了一种基于正面和负面风格翻译示例的新颖风格学习机制。具体流程包括：相关性驱动的代码对选择、功能一致性数据构建、功能学习、风格导向数据构建和风格学习。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
实验中，为了克服现有基准的局限性，如过时的源代码、不足的测试用例和缺失的真实翻译，作者构建了一个新的基准。使用这个新基准以及传统基准，作者在涉及五种编程语言（C、C++、Go、Java和Python）的20种代码翻译场景中评估了F2STRANS。结果表明，F2STRANS在不同类型的LLMs中都是有效的，包括StarCoder3B和Qwen0.5-7B。特别是，使用F2STRANS的方法，Qwen1.5B在代码翻译任务中超过了GPT-4。实验结论是F2STRANS方法在各种类型和规模的LLMs中都能显著提高翻译代码的质量。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
F2STRANS方法可以应用于其他领域，如代码生成、代码修复和上下文感知的代码翻译（CoT）。在代码生成领域，该方法可以帮助生成风格一致且功能正确的代码。在代码修复领域，可以利用功能学习来识别和修复代码中的错误。在CoT领域，风格学习可以帮助模型更好地理解和适应不同编程语言的风格特点。

---

### CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks

**作者**: Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan
**日期**: 2025-07-14
**链接**: http://arxiv.org/abs/2507.10535v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个名为CodeJudgeBench的基准测试，旨在评估大型语言模型（LLM）在编程任务中作为裁判（Judge）的性能。

2. 摘要翻译：
大型语言模型（LLM）在各种编程任务中显著提高了最先进技术的水平。除了直接回答用户查询外，LLM还可以作为裁判，评估和比较其他模型生成的响应质量。这种评估能力对于基准测试不同的LLM以及通过响应排名提高响应质量至关重要。然而，尽管LLM作为裁判的范式日益受到关注，但由于缺乏专门的基准测试，其在编程场景中的有效性尚未得到充分探索。为了解决这一差距，我们引入了CodeJudgeBench，这是一个明确设计的基准测试，用于评估LLM作为裁判在三个关键编程任务中的表现：代码生成、代码修复和单元测试生成。通过对26个LLM作为裁判模型的全面基准测试，我们发现最新的思考模型在精心设计的代码裁判任务上显著优于非思考模型。值得注意的是，即使是相对较小的思考模型，如Qwen3-8B，也能胜过专门训练的高达70B大小的LLM作为裁判模型。尽管如此，所有模型在编程任务的裁判中仍表现出显著的随机性。对于成对裁判任务，简单地改变响应呈现的顺序可能会显著影响准确性。此外，在裁判不同LLM编写的代码和单元测试时，LLM作为裁判模型也显示出性能差异。这种敏感性引发了对LLM作为裁判在编程场景中的可靠性和一致性的担忧。最后，我们研究了LLM作为裁判的最佳提示策略。我们发现，使用成对比较比标量点裁判更有效。此外，在完整的未处理LLM响应中保留注释和推理可以提高裁判性能。

3. 主要贡献和创新点，动机解决和的问题：
主要贡献包括：提出了一个新颖的基准测试CodeJudgeBench，专门用于评估LLM作为裁判在代码生成、代码修复和单元测试生成任务中的表现；全面评估了26个流行的LLM，更全面地揭示了LLM作为裁判在编程任务上的能力；通过各种分析实验，分析了不同因素对LLM作为裁判性能的影响，为开发提供了宝贵的设计建议。动机是探索LLM作为裁判在编程场景中的有效性，解决的问题是缺乏专门的基准测试来评估LLM作为裁判的性能。

4. 方法，具体流程：
CodeJudgeBench的构建涉及三个主要阶段：响应收集、响应验证和响应配对。首先，从LiveCodeBench等平台收集具有挑战性的编程问题。然后，使用最先进的语言模型（如Gemini和Claude）生成候选响应，确保选择和拒绝的响应质量。最后，将这些响应配对，形成CodeJudgeBench数据集中的指令、好的响应和坏的响应三元组。LLM作为裁判的任务是评估这两个响应，并选择更好地满足指令的响应。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：CodeJudgeBench包含4,260个经过策划的配对，涵盖代码生成、代码修复和单元测试生成任务。实验设置：评估了26个流行的LLM，包括开源和闭源模型，以及专门为编程或LLM作为裁判任务调整的模型。实验结果：最新的思考模型在代码裁判任务上显著优于非思考模型。即使是相对较小的思考模型，如Qwen3-8B，也能胜过专门训练的高达70B大小的LLM作为裁判模型。然而，所有模型在编程任务的裁判中仍表现出显著的随机性。实验结论：LLM作为裁判在编程场景中的有效性尚未得到充分验证，存在一定的随机性和不一致性。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
CodeJudgeBench的方法可以应用于其他领域，如Verilog代码生成、代码修复和链式思考（CoT）等编程任务。通过评估LLM作为裁判在这些任务中的表现，可以更好地理解LLM在不同编程领域的能力和局限性，为开发更有效的编程辅助工具提供指导。

---

### CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance

**作者**: Myeongsoo Kim, Shweta Garg, Baishakhi Ray, Varun Kumar, Anoop Deoras
**日期**: 2025-07-14
**链接**: http://arxiv.org/abs/2507.10646v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了CodeAssistBench（CAB），一个用于评估多轮对话式编程辅助的大型语言模型（LLMs）的基准框架，它通过自动化数据生成和模拟真实用户交互来解决现有基准的局限性。

2. 摘要翻译：
编程助手由大型语言模型驱动，已经改变了软件开发，但大多数基准测试过于狭隘地关注代码生成任务。最近的努力，如InfiBench和StackEval，尝试使用Stack Overflow数据来解决这一差距，但它们仅限于单轮交互，在孤立的上下文中，需要大量的手动策划，并且未能代表完整的项目环境。我们介绍了CodeAssistBench（CAB），这是第一个用于评估现实环境中多轮编程辅助的基准框架，它解决了关于实际代码库的真实世界问题。与现有的编程问答基准不同，CAB使用可配置参数（例如，仓库创建日期、星标计数、编程语言）自动生成可扩展的数据集，并包括用于评估的代码库的自动容器化。然后，它通过在这些容器化环境中模拟用户与完整代码库访问权限的模型进行评估。使用这个框架，我们构建了一个测试集，包含3,286个真实世界的编程问题，涵盖231个仓库，跨越七种编程语言和多样的问题领域。我们对领先LLMs的评估揭示了一个巨大的能力差距：虽然模型在Stack Overflow问题上表现良好，成功率为70-83%，但它们只解决了CAB最近问题的16.49%。这种差异突出了在复杂、项目特定的上下文中提供帮助与回答独立问题的挑战。我们的完全自动化框架支持持续的基准扩展，并可在https://anonymous.4open.science/r/CABCBA3/获取。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 一个完全自动化的数据生成框架，无需手动策划即可持续创建和扩展基准。
- 一个包含3,286个真实世界编程问题的多轮问答数据集，涵盖231个仓库，涉及七种编程语言和多样的问题领域，所有这些都来源于实际的GitHub问题。
- 一个新颖的聊天驱动的评估管道，模拟真实用户代理在容器化环境中的交互，具有完整的代码库访问权限。
动机是解决现有基准测试的局限性，如单轮交互、需要大量手动策划、未能代表真实项目环境等问题，以更全面地评估编程助手的能力。解决的问题是如何在复杂、项目特定的上下文中提供有效的编程辅助。

4. 方法，具体流程：
CAB的方法包括两个关键组件：
- 自动化数据生成管道：通过三个主要阶段将GitHub问题转换为结构化的编程辅助场景：仓库收集、问题过滤和数据准备。
- 多代理评估框架：模拟真实用户模型在容器化环境中的交互。
具体流程如下：
- 仓库收集：从GitHub收集代表多种编程挑战的多样化仓库。
- 问题过滤：对每个仓库的问题进行过滤，保留高质量的、互动性强的对话。
- 数据准备：提取构建环境、满意度条件和用户响应参考等结构化数据。
- 评估：通过模拟用户、维护者代理和自动评估器来评估对话质量。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：CAB构建了一个包含3,286个真实世界编程问题的测试集，涵盖231个仓库，涉及七种编程语言和多样的问题领域。
实验设置：评估领先的LLMs在CAB上的表现，与Stack Overflow问题上的表现进行对比。
实验结果：模型在Stack Overflow问题上的成功率为70-83%，但在CAB最近问题上的解决率仅为16.49%。
实验结论：CAB揭示了在复杂、项目特定的上下文中提供编程辅助的挑战，以及CAB框架在生成持续演变的基准方面的价值。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
CAB的方法可以应用于其他领域，如：
- 代码生成：CAB的数据生成和评估框架可以用于生成和评估特定领域（如Verilog）的代码生成模型。
- 代码修复：CAB的多轮对话评估可以用于评估代码修复模型在实际项目环境中的表现。
- CoT（Chain of Thought）：CAB的对话驱动评估可以用于评估CoT模型在解决编程问题时的推理和解释能力。

---

### A Code Comprehension Benchmark for Large Language Models for Code

**作者**: Jayant Havare, Saurav Chaudhary, Ganesh Ramakrishnan, Kaushik Maharajan, Srikanth Tamilselvam
**日期**: 2025-07-14
**链接**: http://arxiv.org/abs/2507.10641v1

1. 一句话介绍论文讲的故事：
这篇论文探讨了大型语言模型在代码理解任务中的表现，并提出了通过在大规模数据集上微调这些模型来提高代码理解能力的方法。

2. 摘要翻译：
大型语言模型（LLMs）在编码任务，如代码生成和代码补全方面展现出了令人印象深刻的能力，因为它们经过了大量的代码数据训练。此外，由于预训练的核心目标之一是下一个词预测，这些模型倾向于学习代码中的表面级语法模式。然而，这并不能保证代码理解能力，即捕捉代码语义的能力。我们认为，这就是为什么这些模型在需要更深层次语义理解的任务上表现不佳，例如代码调试和代码优化。为了解决这个问题，我们提出了针对代码理解任务对这些模型进行微调，使用大规模数据集，使它们能够发展出更强大的代码语义理解能力。我们在一系列设计用于评估超越表面级语法模式匹配的语义理解的代码理解任务上评估了三种不同大小的代码模型。特别是，我们在主观性分级任务上分析了性能，并观察到在相关下游任务上微调后模型性能有所提高。在QWQ-32B模型上看到了最显著的改进，其中准确率从70%提高到83.47%。在其他模型中也观察到了类似或可解释的趋势，清楚地表明了代码理解能力的增强。在研究的模型中，DPO-fine-tuned Codestral-22B在主观性分级任务上达到了最高的微准确度87.66%。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点在于提出了一种通过大规模数据集微调大型语言模型的方法，以提高其在代码理解任务中的表现。动机是现有的大型语言模型虽然在代码生成等任务上表现良好，但在需要深层次语义理解的任务上表现不佳，如代码调试和优化。解决的问题是如何提高模型对代码语义的理解能力，以便更好地执行这些任务。

4. 方法，具体流程：
方法包括选择和设计一系列代码理解任务，这些任务关注代码的结构和语义方面，如主观性分级、问答、语义搜索、测试用例预测、缺陷修复和代码比较。具体流程是：首先，选择和构建适合这些任务的数据集；然后，对模型进行微调，使其能够处理这些任务；最后，在这些任务上评估模型的性能。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集包括CS101-Gold（主观性分级任务）、CodeQA（代码问答任务）等。实验设置包括在这些任务上对三种不同大小的代码模型进行微调和评估。实验结果显示，在微调后，模型在主观性分级任务上的表现有显著提高，其中QWQ-32B模型的准确率从70%提高到83.47%，DPO-fine-tuned Codestral-22B达到了最高的微准确度87.66%。实验结论是，通过在相关下游任务上微调，可以显著提高模型的代码理解能力。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
这种方法可以应用于其他需要深层次代码理解能力的领域，如代码生成（包括Verilog代码生成）、代码修复和代码翻译（CoT）。通过在特定领域的大规模数据集上微调模型，可以提高模型对特定编程语言和代码结构的理解，从而在这些任务上取得更好的性能。

---

### Turning the Tide: Repository-based Code Reflection

**作者**: Wei Zhang, Jian Yang, Jiaxi Yang, Ya Wang, Zhoujun Li, Zeyu Cui, Binyuan Hui, Junyang Lin
**日期**: 2025-07-14
**链接**: http://arxiv.org/abs/2507.09866v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个名为LiveRepoReflection的挑战性基准测试，旨在评估代码大型语言模型（LLMs）在多文件仓库环境中理解和生成代码的能力，并提出了一个基于对话的训练数据集RepoReflection-Instruct，用于提升模型在实际编程任务中的表现。

2. 摘要翻译：
代码大型语言模型（LLMs）通过理解和生成跨语言的代码，提供智能反馈、错误检测和代码更新，从而增强编程能力，提高开发效率和可访问性。尽管已有基准测试（例如HumanEval/LiveCodeBench）评估代码生成和现实世界的相关性，但先前的工作忽略了在仓库中修改代码的场景。考虑到在提高反射能力并避免动态基准测试中的数据污染方面仍存在的挑战，我们引入了LiveRepoReflection，这是一个在多文件仓库环境中评估代码理解和生成的挑战性基准测试，包含1,888个经过严格筛选的测试用例，涵盖6种编程语言，确保多样性、正确性和高难度。此外，我们创建了RepoReflection-Instruct，这是一个大规模、经过质量筛选的指令调整数据集，源自多样化的来源，用于通过两轮对话过程（包括代码生成和错误驱动的修复）训练RepoReflectionCoder。排行榜评估了40多个LLMs，以反映基于仓库的代码反射模型性能。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提出了LiveRepoReflection基准测试，这是一个高质量的、高难度的评估代码反射能力的基准测试，包含1,888个经过严格筛选的测试用例，覆盖6种编程语言。
- 构建了RepoReflection-Instruct，一个大规模的、经过质量筛选的指令调整数据集，用于训练RepoReflectionCoder模型，提升其处理复杂编程任务和迭代交互的能力。
- 通过动态排行榜跟踪模型性能，系统性地评估了40多个LLMs在LiveRepoReflection上的表现，证明了LiveRepoReflection能够有效衡量模型生成响应与人类偏好之间的一致性。
动机和解决的问题：
- 先前的工作忽略了在仓库中修改代码的场景，而LiveRepoReflection旨在填补这一空白，提高LLMs在实际编程环境中的代码反射能力。
- 通过构建高质量的训练数据集和基准测试，解决了动态基准测试中的数据污染问题，提升了模型的实际应用能力。

4. 方法，具体流程：
方法和具体流程包括：
- 收集和筛选编程问题：从Exercism等公共资源中收集编程问题，并去除重复和错误数据，保留约473个编程问题。
- 构建种子代码数据：从GitHub、Hugging Face、Reddit等公共源收集六种编程语言的代码。
- 生成多轮对话数据：通过随机选择的“创造性”LLM生成程序主题和定义，多个“推理”LLM生成单元测试和参考解决方案。
- 交叉执行验证：为每个编程主题生成多个单元测试和参考答案，通过沙箱交叉执行验证，保留最低通过率的单元测试和最高通过率的参考答案。
- 构建LiveRepoReflection基准测试：通过自动化流程生成100K个编程程序仓库案例，运行沙箱测试，筛选出10K个高难度、高正确性的案例。
- 选择困难问题：基于参考答案生成代码签名，组织测试仓库，通过10个主流强推理LLMs的测试结果筛选出高难度问题。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：LiveRepoReflection基准测试包含1,888个经过严格筛选的测试用例，涵盖6种编程语言。
实验设置：通过自动化流程生成100K个编程程序仓库案例，筛选出10K个高难度、高正确性的案例，然后通过10个主流强推理LLMs的测试结果进一步筛选。
实验结果：LiveRepoReflection基准测试评估了40多个LLMs，创建了一个动态排行榜来跟踪模型性能，实验结果表明LiveRepoReflection能够有效衡量模型生成响应与人类偏好之间的一致性。
实验结论：LiveRepoReflection基准测试能够有效评估LLMs在多文件仓库环境中的代码理解和生成能力，为模型性能提供了一个动态的衡量标准。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
LiveRepoReflection基准测试和RepoReflection-Instruct数据集的方法可以应用于其他领域，如：
- 代码生成：可以用于生成特定领域的代码，例如Verilog代码生成，通过调整训练数据集和基准测试用例，使其适应特定领域的代码结构和语法。
- 代码修复：可以用于训练模型识别和修复代码中的错误，通过生成包含错误和修复方案的对话数据，提升模型的代码修复能力。
- CoT（Chain of Thought）：可以用于训练模型进行逐步推理和解释，通过构建包含

---

### A Mixture of Linear Corrections Generates Secure Code

**作者**: Weichen Yu, Ravi Mangal, Terry Zhuo, Matt Fredrikson, Corina S. Pasareanu
**日期**: 2025-07-13
**链接**: http://arxiv.org/abs/2507.09508v1

1. 一句话介绍论文讲的故事：
这篇论文讲述了如何通过一种混合线性校正（MoC）的方法，引导大型语言模型（LLMs）生成更安全的代码，同时不牺牲代码的功能。

2. 摘要翻译：
大型语言模型（LLMs）在复杂的代码生成任务上表现出了熟练的技能，但在可靠地检测或避免代码漏洞方面仍然无效。这种不足是因为对代码漏洞的学习不足，还是因为提示方法无效？我们使用表示工程技术来研究LLMs是否在内部编码了识别代码漏洞所需的概念。我们发现当前的LLMs确实编码了精确的内部表示，能够区分漏洞和安全代码，其准确性超过了标准提示方法。利用这些对漏洞敏感的表示，我们开发了一种在推理时微调模型的令牌生成概率的技术，通过混合校正（MoC）来微妙地调节。我们的方法有效地引导LLMs生成漏洞更少的代码，而不影响功能，展示了一种实际的控制生成代码中漏洞的方法。值得注意的是，MoC将Qwen2.5-Coder-7B的安全比率提高了8.9%，同时在HumanEval pass@1上提高了功能2.1%。代码可在GitHub上找到。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点在于提出了一种混合线性校正（MoC）的方法，通过在推理时调整LLMs的令牌生成概率，引导模型生成更安全的代码。动机是解决LLMs在生成代码时存在的安全漏洞问题，同时保持代码的功能不受影响。解决的问题是LLMs在生成代码时无法有效地识别和避免安全漏洞。

4. 方法，具体流程：
方法的具体流程包括：
- 使用线性探测（linear probing）技术训练一组线性探针，用于代码漏洞检测。
- 通过四种方法之一获得一组线性校正，这些校正直接从聚类、线性探针或辅助神经网络计算得出。
- 将这些校正集成到模型的令牌生成过程中，应用条件校正，并使用时间衰减来微妙地调整下一个令牌的概率，基于从线性探针评估的漏洞。
- 通过这种方式，可以精细、可控地引导生成过程远离漏洞代码，同时避免干扰不太可能产生漏洞代码的生成，从而不牺牲功能。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
实验使用了CodeLlama和Qwen2.5Coder模型，通过比较传统提示技术和使用探测技术后的准确性，发现使用探测技术可以显著提高准确性。实验设置包括在HumanEval pass@1上测试功能正确性，并在Qwen2.5-Coder上测试安全比率。实验结果显示，MoC方法将Qwen2.5-Coder的安全比率提高了8.9%，同时在HumanEval pass@1上提高了功能2.1%。实验结论是MoC方法不仅显著提高了代码的安全性，而且经常增强了代码的功能正确性。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
MoC方法可以应用于其他领域，如：
- 代码生成：MoC方法可以用于生成其他类型的代码，例如Verilog代码，通过训练针对特定语言的线性探针和校正，引导模型生成更安全的代码。
- 代码修复：MoC方法可以用于修复现有代码中的安全漏洞，通过在推理时调整代码生成概率，引导模型生成更安全的代码修复方案。
- CoT（Chain-of-Thought）：MoC方法可以与CoT结合使用，通过在推理时引导模型沿着更安全的路径进行思考和生成代码，提高代码的安全性。

---

### OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique

**作者**: Wasi Uddin Ahmad, Somshubra Majumdar, Aleksander Ficek, Sean Narenthiran, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Vahid Noroozi, Boris Ginsburg
**日期**: 2025-07-11
**链接**: http://arxiv.org/abs/2507.09075v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个名为OpenCodeReasoning-II的新数据集，以及通过自批评测试时扩展方法来提升代码生成和批判性能的研究。

2. 摘要翻译：
近期在基于推理的大型语言模型（LLMs）的发展，尤其是在测试时扩展方面，为代码生成和批判的蒸馏提供了重要机遇。然而，这些领域的发展根本上依赖于大规模、高质量的数据集。在这项工作中，我们介绍了OpenCodeReasoning-II数据集，包含250万个问题-解决方案-批判三元组（约35,000个独特的编程问题），使其成为迄今为止最大的公开可用代码推理数据集。我们采用了两阶段的监督微调策略。第一阶段专注于代码生成的微调，第二阶段涉及模型的联合训练，以进行代码生成和批判。我们得到的Qwen2.5-Instruct模型在代码生成方面的性能超过了或等于最佳先前的开放权重蒸馏模型。值得注意的是，我们的代码生成和批判模型的集成在竞争性编码性能上带来了显著提升。此外，我们提出了LiveCodeBench基准的扩展，特别支持C++编程语言，从而促进使用此基准进行更全面的LLM评估。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献包括：(1) 提供了OpenCodeReasoning-II数据集，这是迄今为止最大的公开可访问的代码推理数据集，包含1.4百万Python和1.1百万C++解决方案及其对应的批判标签、详细的推理痕迹和执行通过率；(2) 扩展了LiveCodeBench基准，特别支持C++，以促进LLM在C++编程语言中的更全面评估；(3) 通过两阶段微调Qwen2.5-Instruct模型，展示了OpenCodeReasoning-II的有效性，使得模型在代码生成性能上超越或匹配领先的开放权重蒸馏模型，并通过简单的测试时扩展策略在LiveCodeBench基准上取得了显著提升；(4) 进行了深入分析，提供了关于测试时扩展下自批评方法的机会、数据扩展的影响以及Python和C++语言之间的迁移的见解。
动机和解决的问题：解决的问题是如何有效地扩展测试时计算，以提升LLMs在代码生成和批判任务上的性能，特别是在复杂的任务如竞争性编码上。

4. 方法，具体流程：
方法包括两阶段的监督微调策略。第一阶段是代码生成的微调，第二阶段是模型的联合训练，以进行代码生成和批判。具体流程如下：
- 从多个来源编译竞争性编程问题和单元测试。
- 使用具有推理能力的LLM生成解决方案。
- 使用推理能力的LLM生成这些解决方案的批判。
- 获得部分生成解决方案的执行结果。
- 在第二和第三阶段，对生成的解决方案及其对应的批判进行后处理，以确保结构一致性。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：OpenCodeReasoning-II，包含250万个问题-解决方案-批判三元组，源自约35,000个独特的编程问题。
实验设置：使用7B、14B和32B参数的模型进行两阶段微调。
实验结果：Qwen2.5-Instruct模型在代码生成方面的表现超过了或等于最佳先前的开放权重蒸馏模型，并且在LiveCodeBench基准上取得了显著提升。
实验结论：通过自批评测试时扩展方法，可以在代码生成和批判任务上实现显著的性能提升。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
该方法可以应用于其他领域，如Verilog代码生成、代码修复和链式思考（CoT）任务。由于该方法依赖于大规模数据集和推理能力，因此可以扩展到需要复杂逻辑推理和代码生成的其他编程语言和任务。

---

### On Evaluating Performance of LLM Inference Serving Systems

**作者**: Amey Agrawal, Nitin Kedia, Anmol Agarwal, Jayashree Mohan, Nipun Kwatra, Souvik Kundu, Ramachandran Ramjee, Alexey Tumanov
**日期**: 2025-07-11
**链接**: http://arxiv.org/abs/2507.09019v1

1. 一句话介绍论文讲的故事：
这篇论文讲述了如何系统地评估大型语言模型（LLM）推理系统的性能，并提出了一套新的评估方法和框架，以解决现有评估方法中存在的问题。

2. 摘要翻译：
大型语言模型（LLM）推理系统的快速发展带来了显著的性能和效率提升。然而，我们的系统分析揭示了当前评估方法常常存在根本性缺陷，通常表现为常见的评估反模式，这些反模式掩盖了真实的性能特征，阻碍了科学进步。通过对近期系统的全面检查，我们识别出在三个关键维度上反复出现的反模式：基线公平性、评估设置和度量设计。这些反模式对于LLM推理来说特别成问题，因为它结合了不同的预填充和解码操作，处理高度异构的工作负载，并且对交互式使用的严格时间要求。我们展示了常见的反模式——如不充分的基线比较混淆了工程努力与算法新颖性，工作负载选择未能代表生产场景，以及度量归一化隐藏了显著的性能变化如生成停滞——如何导致误导性结论。为了应对这些挑战，我们提供了一个从我们分析中得出的全面检查表，建立了一个框架，用于识别和避免这些反模式，以支持稳健的LLM推理评估。为了演示我们框架的实际应用，我们提出了一个案例研究，分析了推测解码技术，这种技术的突发性、非均匀的标记生成在使用这些反模式特征的方法评估时容易被误解。我们的工作为评估方法建立了一个严格的基础，使有意义的比较成为可能，确保可重复的结果，并最终通过超越常见的反模式，将评估与现实世界需求对齐，加速LLM推理系统的真实进展。

3. 主要贡献和创新点，动机和解决的问题：
论文的主要贡献和创新点在于提出了一套新的评估框架和检查表，用于识别和避免在LLM推理系统评估中常见的反模式。动机是现有评估方法存在缺陷，无法准确反映系统的真实性能，阻碍了科学进步。解决的问题是现有评估方法在基线公平性、评估设置和度量设计上的不足，导致评估结果不准确和误导。

4. 方法，具体流程：
论文提出了一个系统框架，包括以下几个步骤：
1) 对LLM推理系统的评估方法进行全面分析，识别常见的反模式；
2) 提供一个全面的检查表，指导研究人员在工作负载选择和度量设计等方面进行关键考虑；
3) 以推测解码技术为例，展示如何应用评估原则揭示被传统方法掩盖的性能特征。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
论文没有提供具体的实验结果数据。但是，它提到了使用Azure LLM推理2024数据集和Mooncake数据集进行分析，并展示了不同应用（如代码和对话）之间的请求长度分布差异。实验设置包括对推测解码技术的评估，以展示如何避免评估反模式揭示关键性能权衡。实验结论是，通过避免评估反模式，可以揭示传统方法掩盖的关键性能特征。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
论文提出的评估框架和检查表可以应用于其他需要评估LLM推理性能的领域，如代码生成（包括Verilog代码生成）、代码修复和上下文感知（CoT）任务。这些领域同样需要准确评估模型性能，以确保模型的有效性和可靠性。通过应用这套评估方法，可以在这些领域中识别和避免评估反模式，从而获得更准确的性能评估结果。

---

### BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity

**作者**: Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun
**日期**: 2025-07-11
**链接**: http://arxiv.org/abs/2507.08771v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一种新型的Mixture-of-Experts（MoE）架构，名为BlockFFN，旨在提高大型语言模型（LLMs）在资源受限的终端设备上的加速性能，通过实现块级别的激活稀疏性。

2. 摘要翻译：
为了减轻大型语言模型（LLMs）的计算负担，具有激活稀疏性的架构，尤其是混合专家（MoE），越来越受到关注。然而，传统的MoE模型由于其不可微分和不灵活的路由机制，影响了模型性能。此外，尽管每个token只激活少量参数，这些稀疏激活的架构在块级别上显示出低稀疏性，意味着多个连续token的联合激活了大量参数。这种稀疏模式在资源受限条件下（例如，终端设备）不利于加速，并且与主流加速技术（例如，推测性解码）不兼容。为了解决这些挑战，我们引入了一种新型MoE架构BlockFFN，以及其高效的训练和部署技术。具体来说，我们使用集成了ReLU激活和RMSNorm的路由器进行可微分和灵活的路由。接下来，为了促进token级别稀疏性（TLS）和块级别稀疏性（CLS），我们设计了CLS感知的训练目标，使BlockFFN更加适合加速。最后，我们实现了高效的加速内核，首次结合了激活稀疏性和推测性解码。实验结果表明，BlockFFN在性能上优于其他MoE基线，实现了超过80%的TLS和70%的8-token CLS。我们的内核在真实终端设备上比密集模型快3.67倍。所有代码和检查点都公开可用。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提出了一种新型MoE架构BlockFFN，通过集成ReLU激活和RMSNorm实现可微分和灵活的路由。
- 设计了CLS感知的训练目标，以提高模型的块级别稀疏性，使其更适合加速。
- 实现了高效的加速内核，首次结合激活稀疏性和推测性解码，显著提高了终端设备上的加速性能。
动机和解决的问题：
- 解决了传统MoE模型由于不可微分和不灵活的路由导致的性能折衷问题。
- 针对低资源条件下终端设备部署大型MoE模型的挑战，提高了块级别稀疏性，使其更适应加速技术。

4. 方法，具体流程：
BlockFFN的方法和流程包括：
- 使用集成ReLU和RMSNorm的路由器，实现可微分和灵活的路由机制。
- 设计CLS感知的训练目标，包括激活局部性损失和块稀疏化损失，以提高模型的块级别稀疏性。
- 实现高效的加速内核，结合激活稀疏性和推测性解码，利用高CLS水平诱导的多个token之间的高激活相似性，减少内存访问量，并基于CUTLASS实现内核，利用张量核心提高计算效率。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
实验结果部分，论文没有提供具体的数据集和实验设置细节，但提到了实验结果：
- BlockFFN实现了超过80%的token级别稀疏性和70%的8-token块级别稀疏性。
- 在真实终端设备上，BlockFFN的内核比基线自回归（AR）解码快3.67倍。
实验结论是BlockFFN在性能上优于其他MoE基线，并在终端设备上实现了显著的加速性能。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
BlockFFN的方法可以应用于其他领域，包括但不限于：
- 代码生成：由于BlockFFN能够处理大型模型并提高计算效率，它可以用于生成高质量的代码，尤其是对于需要大量计算资源的Verilog代码生成。
- 代码修复：BlockFFN的稀疏激活特性可以用于识别和修复代码中的错误或低效部分。
- CoT（Chain of Thought）：BlockFFN可以用于CoT任务，通过其灵活的路由和稀疏激活机制，更有效地处理复杂的推理链。

---

### Multilingual Multimodal Software Developer for Code Generation

**作者**: Linzheng Chai, Jian Yang, Shukai Liu, Wei Zhang, Liran Wang, Ke Jin, Tao Sun, Congnan Liu, Chenchen Zhang, Hualei Zhu, Jiaheng Liu, Xianjie Wu, Ge Zhang, Tianyu Liu, Zhoujun Li
**日期**: 2025-07-11
**链接**: http://arxiv.org/abs/2507.08719v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个名为M2-CODER的多语言多模态软件开发者，它能够结合文本指令和视觉设计输入（如UML图和流程图）来提高代码生成的准确性和与架构对齐。

2. 摘要翻译：
大型语言模型（LLMs）的快速发展显著提高了代码生成能力，但大多数模型仅处理文本输入，忽略了实际软件开发中使用的重要视觉辅助工具，如图表和流程图。为了弥补这一差距，我们引入了M2-CODER，一个多语言多模态软件开发者。M2-CODER整合了视觉设计输入——统一建模语言（UML）图和流程图（称为视觉工作流）——以及文本指令，以增强代码生成的准确性和架构对齐。为此，我们开发了M2C-INSTRUCT，一个包含视觉工作流基础代码生成的多样化多模态指令调整数据集，使M2-CODER能够像人类开发者一样综合文本和图形信息，与以往仅针对狭窄任务的工作不同。此外，我们引入了M2EVAL，一个新的多模态代码生成评估基准，解决了现有文本限制的问题。我们的评估使用M2EVAL突出了模型在精确视觉信息捕获、指令遵循和高级编程知识方面的重大挑战。我们的工作旨在通过使LLMs能够解释和实现通过文本和视觉设计传达的复杂规范，来彻底改变工业编程。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提出了M2C-INSTRUCT，一个包含超过1310万个样本的大规模多语言多模态代码生成数据集，覆盖50种编程语言。
- 提出了M2-CODER，这是一个新颖的多模态代码生成模型，它在M2C-INSTRUCT上训练，利用视觉设计输入（如UML图和流程图）来提高代码生成的准确性和与架构意图的对齐。
- 引入了M2EVAL，一个新的多模态代码生成评估基准，显著扩展了这一领域的评估范围，包括更广泛的编程语言和多样化的任务类型。
动机和解决的问题是：现有的代码生成模型主要处理文本输入，忽略了实际软件开发中重要的视觉辅助工具，限制了它们在需要视觉上下文的场景中的有效性。M2-CODER旨在弥合现有代码生成模型与人类软件开发者之间在理解复杂规范方面的鸿沟。

4. 方法，具体流程：
M2-CODER的方法包括两个阶段的训练：首先在多样化的大规模多模态代码相关指令样本上进行预训练，然后在强调视觉工作流基础代码生成的高质量指令语料库上进行微调。具体流程如下：
- 设计原型问题：基于常见编程概念设计Python原型问题，LLM生成初始提示、解决方案和测试用例，然后手动迭代优化。
- 问题设计：为每个原型问题生成图表（PlantUML/Mermaid），手动优化图表的结构和语义准确性，并修订问题提示，确保仅提供提示不足以正确解决问题。
- 翻译到其他编程语言：将Python问题翻译成其他九种编程语言，调整提示、解决方案和测试用例，确保一致性。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集：M2EVAL包含300个问题，涵盖30个独特概念，每种概念在10种编程语言中都有实例，共30个并行实例。问题描述平均89个token，最大388个。包含30个不同的图像，每个概念一个，所有10种编程语言版本共享。解决方案平均326个token，最大826个，每个解决方案平均有9个测试用例。
实验设置：使用M2EVAL评估M2-CODER和其他流行的LMMs，比较它们在精确视觉理解、指令遵循和应用高级编程概念方面的表现。
实验结果：7B参数的M2-CODER在竞争力上与70B+参数的LMMs相当。分析发现LMMs在精确视觉理解、指令遵循和应用高级编程概念方面存在持续的局限性，为该领域提供了宝贵的研究方向。
实验结论：M2-CODER在多模态代码生成方面取得了显著进展，但仍面临视觉信息捕获、指令遵循和高级编程知识应用的挑战。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
M2-CODER的方法可以应用于其他领域，如：
- 代码生成：尤其是对于需要结合视觉设计输入（如电路图）的Verilog代码生成，M2-CODER的方法可以提高代码的准确性和与设计意图的对齐。
- 代码修复：通过理解代码的视觉表示和文本指令，M2-CODER可以帮助识别和修复代码中的错误。
- Co

---

### Agentic Large Language Models for Conceptual Systems Engineering and Design

**作者**: Soheyl Massoudi, Mark Fuge
**日期**: 2025-07-11
**链接**: http://arxiv.org/abs/2507.08619v1

1. 一句话介绍论文讲的故事：
这篇论文探讨了如何利用代理型大型语言模型（LLM）在概念性系统工程和设计中进行需求提取、功能分解和模拟器代码生成。

2. 摘要翻译：
早期的工程设计涉及复杂的迭代推理，但现有的大型语言模型（LLM）工作流程难以维持任务连续性并生成可执行模型。我们评估了一个结构化的多代理系统（MAS）是否比一个更简单的双代理系统（2AS）更有效地管理需求提取、功能分解和模拟器代码生成。目标应用是一个太阳能驱动的水过滤系统，如任务书中所述。我们引入了设计状态图（DSG），这是一种可JSON序列化的表示，将需求、物理体现和基于Python的物理模型捆绑到图节点中。一个九角色的MAS迭代构建和完善DSG，而2AS将过程简化为生成器-反射器循环。两个系统共运行60次实验（2个LLM - Llama 3.3 70B vs 推理蒸馏DeepSeek R1 70B x 2代理配置x 3温度x 5种子）。我们报告了JSON有效性、需求覆盖率、体现存在、代码兼容性、工作流完成、运行时间和图大小。在所有运行中，MAS和2AS都保持了完美的JSON完整性和体现标记。需求覆盖率保持在最低水平（不到20%）。在特定的2AS设置下，代码兼容性达到了100%，但MAS的平均值低于50%。只有推理蒸馏模型可靠地标记了工作流的完成。由DeepSeek R1 70B驱动的MAS生成了更细粒度的DSG（平均5-6个节点），而2AS模式崩溃。结构化的多代理协调增强了设计细节。推理蒸馏LLM提高了完成率，但在编码中持续存在低需求和保真度差距。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提出了一个结构化的多代理系统（MAS）来管理工程设计中的复杂任务，如需求提取、功能分解和模拟器代码生成。
- 引入了设计状态图（DSG），这是一种新的JSON序列化表示，可以将需求、物理体现和基于Python的物理模型整合到图节点中。
- 通过对比MAS和双代理系统（2AS）的性能，评估了结构化多代理框架在工程设计中的优势。

动机和解决的问题：
- 工程设计是一个复杂、迭代和多目标的过程，现有的LLM工作流程难以适应早期设计探索和需求提取的动态、迭代和模糊性质。
- 通过引入代理型LLM，旨在提高工程设计的自动化水平，特别是在需求提取和功能分解方面。

4. 方法，具体流程：
方法包括：
- 使用大型语言模型（LLM）作为自主AI代理，通过规划、记忆、工具使用和行动执行四个基本能力来增强LLM。
- 利用Chat Markup Language（ChatML）定义的结构化消息序列，实现LLM与外部系统的交互。
- 通过JSON序列化的设计状态图（DSG）来表示和处理设计元素。
- 实施一个九角色的MAS来迭代构建和完善DSG，而2AS则简化为生成器-反射器循环。

具体流程：
- 定义系统消息，包括个性特征、领域专业化和输出格式。
- 用户输入消息，代表外部实体的输入。
- LLM生成的助手消息，可能包括直接文本回复或结构化工具调用。
- 工具消息，用于将工具执行的结果传回LLM，使其能够将外部数据整合到推理中。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
数据集和实验设置：
- 两个LLM（Llama 3.3 70B和推理蒸馏DeepSeek R1 70B）。
- 两种代理配置（MAS和2AS）。
- 三种温度设置和五种种子。
- 共进行60次实验。

实验结果：
- 两种系统都保持了完美的JSON完整性和体现标记。
- 需求覆盖率保持在最低水平（不到20%）。
- 在特定2AS设置下，代码兼容性达到了100%，但MAS的平均值低于50%。
- 只有推理蒸馏模型可靠地标记了工作流的完成。
- MAS生成了更细粒度的DSG（平均5-6个节点），而2AS模式崩溃。

实验结论：
- 结构化的多代理协调增强了设计细节。
- 推理蒸馏LLM提高了完成率，但在编码中持续存在低需求和保真度差距。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
这篇论文提出的方法可以应用于其他领域，例如：
- 代码生成：由于LLM能够处理自然语言和

---

### Automating MD simulations for Proteins using Large language Models: NAMD-Agent

**作者**: Achuth Chandrasekhar, Amir Barati Farimani
**日期**: 2025-07-10
**链接**: http://arxiv.org/abs/2507.07887v1

1. 一句话介绍论文讲的故事：
这篇论文介绍了一个利用大型语言模型（LLMs）自动化分子动力学（MD）模拟输入文件生成的管道，特别是针对蛋白质模拟，通过结合Python脚本和基于Selenium的网页自动化来简化NAMD的输入文件准备过程。

2. 摘要翻译：
分子动力学模拟是理解蛋白质结构、动态和功能的重要工具。然而，为MD模拟准备高质量的输入文件是一个耗时且容易出错的过程。在这项工作中，我们引入了一个自动化管道，利用大型语言模型（LLMs），特别是Gemini-2.0-Flash，结合Python脚本和基于Selenium的网页自动化，来简化MD输入文件的生成。该管道利用CHARMM-GUI全面的基于网络的界面，为NAMD准备模拟就绪的输入。通过整合Gemini的代码生成和迭代细化能力，自动编写、执行和修订模拟脚本，以导航CHARMM-GUI，提取适当的参数，并生成所需的NAMD输入文件。使用额外的软件进行后处理，以进一步细化模拟输出，从而实现完整且几乎无需人工干预的工作流程。我们的结果表明，这种方法减少了设置时间，最小化了手动错误，并为并行处理多个蛋白质系统提供了可扩展的解决方案。这个自动化框架为LLMs在计算结构生物学中的更广泛应用铺平了道路，为未来模拟自动化的发展提供了一个健壮且可适应的平台。

3. 主要贡献和创新点，动机和解决的问题：
主要贡献和创新点包括：
- 提供了一种自动化管道，利用大型语言模型（LLMs）来自动化分子动力学（MD）模拟的输入文件生成。
- 通过结合Python脚本和基于Selenium的网页自动化，简化了NAMD的输入文件准备过程。
- 利用CHARMM-GUI的网络界面，自动化了模拟就绪输入的准备。
- 通过Gemini的代码生成和迭代细化能力，实现了模拟脚本的自动编写、执行和修订。
- 实现了一个完整且几乎无需人工干预的工作流程，减少了设置时间，最小化了手动错误，并提供了并行处理多个蛋白质系统的可扩展解决方案。

动机和解决的问题：
- 传统的MD模拟输入文件准备过程耗时且容易出错，需要专业知识。
- 该研究旨在通过自动化减少人为错误，提高设置效率，并使MD模拟更加易于访问。

4. 方法，具体流程：
方法包括：
- 使用Gemini-2.0-Flash作为LLMs的骨干，结合LlamaIndex Python框架，支持LLM应用。
- 利用ReAct（Reasoning and Acting）代理，这些智能系统在交互环境中交替执行推理和行动。
- 通过Selenium自动化CHARMM-GUI网页界面，自动生成和清理YAML配置文件，并运行CHARMM-GUI构建器。
- 自动化流程包括下载PDB文件、生成和清理配置文件、运行CHARMM-GUI构建器以及执行和后处理NAMD模拟。

5. 实验结果，包括数据集，实验设置，实验结果，实验结论：
论文中没有提供具体的实验结果、数据集、实验设置和实验结论的详细信息。通常，这类研究会在实验部分详细描述所使用的数据集、实验的具体设置、得到的结果以及从中得出的结论。这些信息对于评估方法的有效性和可靠性至关重要。

6. 方法可以用在其它什么领域，如代码生成（尤其是Verilog代码生成），代码修复，CoT？
该方法可以应用于其他领域，包括：
- 代码生成：利用LLMs生成特定领域的代码，如Verilog代码生成，可以自动化硬件描述语言的编写过程。
- 代码修复：LLMs可以用于理解代码上下文并提出修复建议，从而自动化代码调试和维护过程。
- CoT（Chain of Thought）：LLMs可以用于模拟人类解决问题的思维过程，通过逐步推理来解决复杂问题，这在教育、咨询和其他需要逻辑推理的领域具有潜在应用价值。
